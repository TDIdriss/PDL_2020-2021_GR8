

Extracting via wikitext does not work very well, especially checking tables. Json Format causes problems when extracting a table. This can be seen in issue 1 where the offending part was detected. The oldid parameter which is returned to the API is never modified so always remains null. As a result, the API does not work with the right values, which in our opinion explains the poor data recovery at Wikitext level. In addition, the files loaded in some test are not the right ones from the point of view of the functionality to be evaluated by the test. We therefore had to modify some tests with files containing urls which met the prerequisites of the test methods.

We also noticed that some method did not work correctly like the method the method isPageWikipedia ()
After launching the WikiExtactMain
For HTML, the extraction at the 336 url level provides the following statistics:
 Running time: 317 seconds.
 Number of parsed tables: 1698,
 parsed lines: 36949,
 parsed columns: 15883
For HTML, the extraction at the 336 url level provides the following statistics:

At the level of WIKITEXT, we have the statistics below:
 Running time: 538334171716 seconds.
 Number of arrays parsed: 0,
 parsed lines: 0,
 parsed columns: 0
We notice that the extraction with wikitext did not work. In addition, the execution time is 1,602,185,034 times that of html.
There are therefore code bugs concerning wikitext that we are looking for. Once the bugs have been identified, we will fix them.
In the proofTest, proofhTMLTextExtractor Text and The proofWikiTextExtractor Text checks whether the csvs resulting from the extraction (out / html and out / wikitex) conform to reality (proof.txt file).

 The proofhTMLTextExtractor Text fails when running the test. We inspected the code to be able to identify the source of the failure.
We noticed that the extraction works normally and the csv file is valid. But for some csv files, the information is not the same between the csv generated by the extractor and the one that establishes the ground truth. Precisely at the level of tables which contain cells where there are dates or versions of products which are updated over time (example: when we take the table from the url: https://en.wikipedia.org/ wiki / Comparison_of_debuggers, we can see that the last column contains most recent realese).
In order for this text to pass, it is therefore important to retrieve recent csv tables from within the proof.txt file on the pages before performing the tests.
 The proofWikiTextExtractor Text passes successfully. But after reading the code, we realize that the test was not done well during the coding. Indeed, one should compare the result of the extractor to that which establishes the ground truth. Which is not the case. The test result just relied on the validity of the extracted csv file.
So we completed the rest of the code to be able to make the tests work.

In addition we have made some changes such as code improvements and new method creation. Indeed to check the language of a url you had to first find out if it was a valid url so to know if it is a Wikipedia page and if its title is valid which seemed not intuitive enough to us, we therefore created a method isLanguageValid which tells us if the language is taken into account (English, French) or not.
  